{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning workflow\n",
    "\n",
    "Ever since I started doing machine learning I was torn apart between Python and R. R is extremely easy at the beginning and you might create a simple model in a matter of minutes. \n",
    "\n",
    "Python + Scikit + Pandas has definitely steeper learning curve. At this point, I prefer Python over R as I find it simpler and more explicit. It's harder but if you use it properly it's simpler.\n",
    "\n",
    "If you want to play with the code, go ahead and download the .ipynb source of this blogpost.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The workflow\n",
    "\n",
    "Below I show an example on how nicely you structure your code with the help of `sklearn.pipeline.Pipeline`, `sklearn.preprocessing.*` and `sklearn-pandas` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import csv as csv\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import get_dummies\n",
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.cross_validation import KFold, StratifiedKFold, train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data from Kaggle Titanic competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv', header = 0, index_col = 'PassengerId')\n",
    "df_test = pd.read_csv('test.csv', header = 0, index_col = 'PassengerId')\n",
    "df = pd.concat([df_train, df_test], keys=[\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I concatenated `df_train` and `df_test` above.\n",
    "This simplifies dealing with any with missing data, creating new features and such as I do it once for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Title'] = df['Name'].apply(lambda c: c[c.index(',') + 2 : c.index('.')])\n",
    "df['LastName'] = df['Name'].apply(lambda n: n[0:n.index(',')])\n",
    "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "df.loc[df['Embarked'].isnull(), 'Embarked'] = df['Embarked'].mode()[0]\n",
    "df.loc[df['Fare'].isnull(), 'Fare'] = df['Fare'].mode()[0]\n",
    "df['FamilyID'] = df['LastName'] + ':' + df['FamilySize'].apply(str)\n",
    "df.loc[df['FamilySize'] <= 2, 'FamilyID'] = 'Small_Family'\n",
    "\n",
    "df['AgeOriginallyNaN'] = df['Age'].isnull().astype(int)\n",
    "medians_by_title = pd.DataFrame(df.groupby('Title')['Age'].median()) \\\n",
    "  .rename(columns = {'Age': 'AgeFilledMedianByTitle'})\n",
    "df = df.merge(medians_by_title, left_on = 'Title', right_index = True) \\\n",
    "  .sort_index(level = 0).sort_index(level = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you noticed I used multiindex when creating `df`? Now, it's simple to split it back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df.ix['train']\n",
    "df_test  = df.ix['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of `featurize` function above is to create dummy variables out of categorical ones. In `Scikit` we must be explicit about it as the algorithms accept only `float` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize(features):\n",
    "  transformations = [\n",
    "                            ('Embarked', LabelBinarizer()),\n",
    "                            ('Fare', None),\n",
    "                            ('Parch', None),\n",
    "                            ('Pclass', LabelBinarizer()),\n",
    "                            ('Sex', LabelBinarizer()),\n",
    "                            ('SibSp', None),                                       \n",
    "                            ('Title', LabelBinarizer()),\n",
    "                            ('FamilySize', None),\n",
    "                            ('FamilyID', LabelBinarizer()),\n",
    "                            ('AgeOriginallyNaN', None),\n",
    "                            ('AgeFilledMedianByTitle', None)]\n",
    "\n",
    "  return DataFrameMapper(filter(lambda x: x[0] in df.columns, transformations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['Sex', 'Title', 'FamilySize', 'AgeFilledMedianByTitle',\n",
    "            'Embarked', 'Pclass', 'FamilyID', 'AgeOriginallyNaN']\n",
    "\n",
    "pipeline = Pipeline([('featurize', featurize(features)), ('forest', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pipeline` is a concatenation of transformer `featurize` and classifier `forest`. \n",
    "\n",
    "Note, I used `sklearn-pandas` `DataFrameMapper` adapter to bridge `sklearn` and `pandas` in a seamless way. Now I can put `Pandas` data frames right into the pipeline to fit the model. No awkward jumping from `Pandas` and `SciKit` back and forth!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df_train[df_train.columns.drop('Survived')]\n",
    "y = df_train['Survived']\n",
    "model = pipeline.fit(X = X, y = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can reuse the pipeline for prediction and rely on pipeline to perform label binarizations the same way it was done for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fruits\n",
    "In R, some of these steps would be easier. For example instead of playing with dummy variables, one would simply use `Survived ~ Sex + Title + FamilySize + ... + AgeOriginallyNaN` and the algorithms would treat factor variables properly. \n",
    "\n",
    "In `Scikit` we had to laboriously prepare the data and pipeline. Let's see what laverage does it provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying model\n",
    "Creating a model with different set of features or different classifier is very simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "simple_pipeline = Pipeline([('featurize', featurize(['Sex'])), ('lm', LogisticRegression())])\n",
    "model = simple_pipeline.fit(X = X, y = y).predict(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "Say we want to do k fold cross validation to validate our model. According to [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_score.html#sklearn.cross_validation.cross_val_score) this should perform a 3-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.78114478,  0.8047138 ,  0.81818182])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(pipeline, X, y, 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! How about k-fold stratified cross validation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.82222222,  0.83333333,  0.78651685,  0.87640449,  0.83146067,\n",
       "        0.79775281,  0.82022472,  0.78651685,  0.87640449,  0.85227273])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stratified_kfold = StratifiedKFold(df_train['Survived'] == 1, n_folds = 10)\n",
    "cross_val_score(simple_pipeline, X, y, 'accuracy', cv = stratified_kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is impressive!\n",
    "We can also plug in the previous pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.76666667,  0.8       ,  0.73033708,  0.85393258,  0.87640449,\n",
       "        0.78651685,  0.78651685,  0.76404494,  0.83146067,  0.84090909])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(pipeline, X, y, 'accuracy', cv = stratified_kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters optimization\n",
    "We have 3 algorithms implemented in `Scikit` to choose from. Let's try Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=14, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['Sex', 'Title', 'FamilySize', 'AgeFilledMedianByTitle',\n",
    "            'Embarked', 'Pclass', 'FamilyID', 'AgeOriginallyNaN']\n",
    "\n",
    "pipeline = Pipeline([('featurize', featurize(features)), ('forest', RandomForestClassifier(n_estimators = 10))])\n",
    "\n",
    "pipeline.fit(X, y).predict(df_train)\n",
    "\n",
    "param_grid = dict(forest__n_estimators = [2, 16, 32], forest__criterion = ['gini', 'entropy'])\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid)\n",
    "best_pipeline = grid_search.fit(X, y).best_estimator_\n",
    "best_pipeline.get_params()['forest']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out parameters of the best forest are: 16 estimators and `gini`. \n",
    "\n",
    "Oh wait but how is it measured? Right, the `cv` argument unset, so we are at 3-fold cross-validation. Let's try out stratified cross validation again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=16, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv = stratified_kfold)\n",
    "best_pipeline = grid_search.fit(X, y).best_estimator_\n",
    "best_pipeline.get_params()['forest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to verify that indeed stratified 10-fold cross validation was performed, you can set e.g. `verbose = 10` as argument to `GridSearchCV`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interfacing with R\n",
    "Last but not least structuring the code like I did above gives makes laveraging R using `rpy2` very simple, as you have a R ready variables, mainly `df`, `df_test` and `df_train`. \n",
    "\n",
    "I myself do that to plot using R's excellent `ggplot`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
